{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEGWpf9EUpFK"
      },
      "source": [
        "!git clone https://github.com/Taller-Abierto-de-Humanidades-Digitales/npl.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-XNES7rVGmA"
      },
      "source": [
        "archivo = \"/content/npl/AMLO_transcripciones/2018-12-04.txt\"\n",
        "\n",
        "f = open(archivo, \"r\", encoding='utf-8')\n",
        "texto = f.read()\n",
        "texto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0TjHrY1MAGi"
      },
      "source": [
        "cadenas_extra = [\"PALACIO NACIONAL\", \"\\xa0\", \n",
        "                 \"Conferencia de prensa del presidente de México, Andrés Manuel López Obrador.\", \n",
        "                 \"-PRESIDENTE ANDRÉS MANUEL LÓPEZ OBRADOR:\", \"PRESIDENTE ANDRÉS MANUEL LÓPEZ OBRADOR:\", \n",
        "                 \"– – – 0 – – –© Copyright Derechos Reservados 2011-2020 - Sitio Oficial de Andrés Manuel López Obrador.\",\n",
        "                 \"-PREGUNTA:\", \"(…)\" \n",
        "                 ]\n",
        "\n",
        "\n",
        "for c in cadenas_extra:\n",
        "  texto = texto.replace(c, \" \")\n",
        "\n",
        "texto\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGCB0pzx9GD7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59KmRV-y9Wal"
      },
      "source": [
        "# tokenización de oraciones\n",
        "\n",
        "oraciones = sent_tokenize(texto,\"spanish\")\n",
        "oraciones.pop(0)\n",
        "print(oraciones[:10])\n",
        "print(len(oraciones))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H6JwS5e-J6D"
      },
      "source": [
        "# tokenización de palabras\n",
        "\n",
        "palabras = word_tokenize(texto,\"spanish\")\n",
        "print(palabras[:10])\n",
        "print(len(palabras))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGJmN1AF-Z2g"
      },
      "source": [
        "# fecuencias\n",
        "\n",
        "from nltk.probability import FreqDist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mfk6SRK6-dvb"
      },
      "source": [
        "frec = FreqDist(palabras)\n",
        "\n",
        "frec.most_common(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obpODba_-kCP"
      },
      "source": [
        "# limpiar signos de puntuación\n",
        "\n",
        "palabras_nopunct = []\n",
        "\n",
        "for p in palabras:\n",
        "  if p.isalpha():\n",
        "    palabras_nopunct.append(p.lower())\n",
        "\n",
        "print(len(palabras_nopunct))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLf3ZEj1-wzU"
      },
      "source": [
        "# palabras vacías\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOSFtDDF-3Gk"
      },
      "source": [
        "palabrasv = stopwords.words('spanish')\n",
        "print(palabrasv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoJQlSme-4k_"
      },
      "source": [
        "texto_limpio = []\n",
        "\n",
        "for p in palabras_nopunct:\n",
        "  if p not in palabrasv:\n",
        "    texto_limpio.append(p)\n",
        "\n",
        "print(len(texto_limpio))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XW-iSfE_CnE"
      },
      "source": [
        "# Visualización simple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fre = FreqDist(palabras)\n",
        "fre.most_common(20)\n",
        "fre.plot(20)\n",
        "\n",
        "fre = FreqDist(palabras_nopunct)\n",
        "fre.most_common(20)\n",
        "fre.plot(20)\n",
        "\n",
        "fre = FreqDist(texto_limpio)\n",
        "fre.most_common(20)\n",
        "fre.plot(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zodwTAxQ_fJy"
      },
      "source": [
        "## Visualización Nube de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DqB7CrX_NOq"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "wordcloud = WordCloud(colormap='prism', background_color='white', stopwords=palabrasv)\n",
        "wordcloud = wordcloud.generate(texto)\n",
        "plt.figure(figsize=(20,10), facecolor='k')\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCcRieKbBrJ8"
      },
      "source": [
        "import imageio\n",
        "mask_image = imageio.imread(\"https://github.com/pdeitel/IntroToPython/raw/master/examples/ch12/mask_star.png\")\n",
        "#mask_image = imageio.imread('https://github.com/pdeitel/IntroToPython/raw/master/examples/ch12/mask_heart.png')\n",
        "\n",
        "# círculo \"https://github.com/pdeitel/IntroToPython/raw/master/examples/ch12/mask_circle.png\"\n",
        "# óvalo \"https://github.com/pdeitel/IntroToPython/raw/master/examples/ch12/mask_oval.png\"\n",
        "# estrella \"https://github.com/pdeitel/IntroToPython/raw/master/examples/ch12/mask_star.png\"\n",
        "\n",
        "\n",
        "wordcloud = WordCloud(colormap='prism', background_color='white', stopwords=palabrasv, mask=mask_image)\n",
        "wordcloud = wordcloud.generate(texto)\n",
        "plt.figure(figsize=(20,10), facecolor='k')\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5MmvGdGEgqc"
      },
      "source": [
        "# Derivación regresiva (Word Stemming)\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "raices = SnowballStemmer('spanish')\n",
        "\n",
        "print(raices.stem(\"canciones\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdAO4972E-M2"
      },
      "source": [
        "for p in texto_limpio[:10]:\n",
        "  print(raices.stem(p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ7cTKODHE-1"
      },
      "source": [
        "# reconocimiento de entidades\n",
        "!python -m spacy download es\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('es')\n",
        "\n",
        "documento = nlp(texto)\n",
        "\n",
        "for entidad in documento.ents:\n",
        "  print(f\"{entidad.text}: {entidad.label_}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjdwYpm8H_hC"
      },
      "source": [
        "# comparar similitudes\n",
        "\n",
        "archivo = \"/content/npl/AMLO_transcripciones/2021-04-13.txt\"\n",
        "\n",
        "f = open(archivo, \"r\", encoding='utf-8')\n",
        "texto2 = f.read()\n",
        "for c in cadenas_extra:\n",
        "  texto2 = texto2.replace(c, \" \")\n",
        "\n",
        "texto2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfQjWzitIxRN"
      },
      "source": [
        "documento1 = nlp(texto)\n",
        "documento2 = nlp(texto2)\n",
        "\n",
        "documento1.similarity(documento2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX0IATP5JRBm"
      },
      "source": [
        "#TextBlob\n",
        "\n",
        "## TRaducción\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(texto)\n",
        "\n",
        "blob.detect_language()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iR8aW75JnL5"
      },
      "source": [
        "english = blob.translate(to='en')\n",
        "english\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCl0t4ewKFqZ"
      },
      "source": [
        "# Análisis de sentimientos\n",
        "\n",
        "for s in english.sentences:\n",
        "  print(f\"{s} = {s.sentiment}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ADQ_iNhK21_"
      },
      "source": [
        "#Análisis de sentimientos con NaiveBayesAnalyzer\n",
        "nltk.download('movie_reviews')\n",
        "from textblob.sentiments import NaiveBayesAnalyzer\n",
        "\n",
        "blob = TextBlob(str(english), analyzer=NaiveBayesAnalyzer())\n",
        "\n",
        "for s in blob.sentences:\n",
        "  print(f\"{s[:20]} = {s.sentiment}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}